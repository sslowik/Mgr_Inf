getwd()
setwd("H:/Mgr_Inf/Rstudio/LAB7")
getwd()
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust",
"cluster", "igraph", "fpc")
install.packages(Needed, dependencies = TRUE)
cname <- file.path(getwd(), "texts")
cname
dir(cname)
library(tm)
docs <- VCorpus(DirSource(cname))
summary(docs)
inspect(docs[1])
inspect(docs[2])
writeLines(as.character(docs[1]))
docs <- tm_map(docs,removePunctuation)
for (j in seq(docs)) {
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
docs[[j]] <- gsub("\u2028", " ", docs[[j]])
}
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, PlainTextDocument)
DocsCopy <- docs
# For a list of the stopwords, see:
length(stopwords("english"))
stopwords("english")
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, PlainTextDocument)
stopwords("polish")
.libPaths()
polish <- scan("stopwords-pl.txt", character(), quote = "")
docs <- tm_map(docs, removeWords, c("syllogism", "tautology"))
class(polish)
stopwords("polish")
# removing my own words
# i decided to paste poish.dat file into library folder:
.libPaths()
#1.b.7. removing specific words:
#vector with my words:
wywal_je <- c("my","custom","words")
docs <- tm_map(docs, removeWords, wywal_je)
docs <- tm_map(docs, removeWords, stopwords("polish"))
docs <- tm_map(docs, PlainTextDocument)
writeLines(as.character(docs[1]))
summary(docs)
inspect(docs[1])
install.package("stopwords")
install.packages("stopwords")
library(stopwords)
wywal_je <- as.vector(stopwords(language = "en", source = "smart"))
wywal_je
docs <- tm_map(docs, removeWords, wywal_je)
docs <- tm_map(docs, PlainTextDocument)
summary(docs)
inspect(docs[1])
inspect(docs[2])
wywal_je <- as.vector(stopwords(language, source = "misc"))
wywal_je <- as.vector(stopwords(language = "en", source = "misc"))
wywal_je <- as.vector(stopwords(language = "english", source = "misc"))
wywal_je <- as.vector(stopwords(language, source = "stopwords-iso")
)
wywal_je <- as.vector(stopwords(language, source = "stopwords-iso"))
wywal_je <- as.vector(stopwords("english", source = "stopwords-iso"))
docs <- tm_map(docs, removeWords, wywal_je)
docs <- tm_map(docs, PlainTextDocument)
inspect(docs[1])
inspect(docs[2])
wywal_je <- as.vector(stopwords("english", source = "snowball"))
docs <- tm_map(docs, removeWords, wywal_je)
docs <- tm_map(docs, PlainTextDocument)
inspect(docs[1])
inspect(docs[2])
wywal_je
for (j in seq(docs))
s
ad
for (j in seq(docs)) {
docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
docs[[j]] <- gsub("inner city", "inner-city", docs[[j]])
docs[[j]] <- gsub("politically correct", "politically_correct", docs[[j]])
}
docs <- tm_map(docs, PlainTextDocument)
docs_st <- tm_map(docs, stemDocument)
docs_st <- tm_map(docs_st, PlainTextDocument)
writeLines(as.character(docs_st[1]))
writeLines(as.character(docs[1]))
docs <- tm_map(docs, removeWords, "list()", ")", "(", " = ")
docs <- tm_map(docs, removeWords, c("list()", ")", "(", " = "))
docs <- tm_map(docs, PlainTextDocument)
writeLines(as.character(docs[1]))
docs <- tm_map(docs, removeWords, c("()", ")", "(", " = "))
docs <- tm_map(docs, PlainTextDocument)
writeLines(as.character(docs[1]))
docs <- tm_map(docs, removeWords, c("()", ")", "(", " = "),"musã©","â\200“")
docs <- tm_map(docs, removeWords, c("()", ")", "(", " = ","musã©","â\200“"))
docs <- tm_map(docs, PlainTextDocument)
inspect(docs[1])
writeLines(as.character(docs[1]))
docs <- tm_map(docs,removePunctuation)
docs <- tm_map(docs, removeNumbers)
for (j in seq(docs)) {
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
docs[[j]] <- gsub("\u2028", " ", docs[[j]])
docs[[j]] <- gsub("(", " ", docs[[j]])
docs[[j]] <- gsub(")", " ", docs[[j]])
}
for (j in seq(docs)) {
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
docs[[j]] <- gsub("\u2028", " ", docs[[j]])
docs[[j]] <- gsub("\(", " ", docs[[j]])
docs[[j]] <- gsub("\)", " ", docs[[j]])
}
for (j in seq(docs)) {
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
docs[[j]] <- gsub("\u2028", " ", docs[[j]])
}
for (j in seq(docs)) {
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
docs[[j]] <- gsub("\u2028", " ", docs[[j]])
docs[[j]] <- gsub("(", " ", docs[[j]])
}
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, PlainTextDocument)
DocsCopy <- docs
# For a list of the stopwords, see:
length(stopwords("english"))
stopwords("english")
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, PlainTextDocument)
docs <- tm_map(docs, removeWords, c("syllogism", "tautology"))
docs <- tm_map(docs, removeWords, stopwords("polish"))
docs <- tm_map(docs, PlainTextDocument)
wywal_je <- as.vector(stopwords(language = "en", source = "smart"))
docs <- tm_map(docs, removeWords, wywal_je)
docs <- tm_map(docs, PlainTextDocument)
inspect(docs[1])
wywal_je <- as.vector(stopwords("english", source = "stopwords-iso"))
docs <- tm_map(docs, removeWords, wywal_je)
docs <- tm_map(docs, PlainTextDocument)
inspect(docs[1])
writeLines(as.character(docs[1]))
docs <- tm_map(docs, removeWords, c("()", ")", "(", " = ","musã©","â\200“"))
docs <- tm_map(docs, removeWords, c("()", ")", "(", " = ","musã©","â\200“"))
docs <- tm_map(docs, PlainTextDocument)
# Combining words that should stay together
for (j in seq(docs)) {
docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
docs[[j]] <- gsub("inner city", "inner-city", docs[[j]])
docs[[j]] <- gsub("politically correct", "politically_correct", docs[[j]])
}
docs <- tm_map(docs, PlainTextDocument)
docs_st <- tm_map(docs, stemDocument)
docs_st <- tm_map(docs_st, PlainTextDocument)
writeLines(as.character(docs_st[1]))
docs_stc <- tm_map(docs_st, stemCompletion, dictionary = DocsCopy, lazy=TRUE)
docs_stc <- tm_map(docs_stc, PlainTextDocument)
writeLines(as.character(docs_stc[1]))
tdm <- TermDocumentMatrix(docs)
tdm
dtm <- DocumentTermMatrix(docs)
dtm
m <- as.matrix(dtm)
dim(m)
write.csv(m, file="DocumentTermMatrix.csv")
dtms <- removeSparseTerms(dtm, 0.2)
dtms
freq <- colSums(as.matrix(dtm))
length(freq)
head(table(freq), 20)
tail(table(freq), 20)
freq <- colSums(as.matrix(dtms))
freq
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
head(freq, 14)
findFreqTerms(dtm, lowfreq=50)
wf <- data.frame(word=names(freq), freq=freq)
head(wf)
library(ggplot2)
p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
p
freq
p <- ggplot(subset(wf, freq>10), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
p
p2 <- ggplot(subset(wf, freq>10), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
p2
findAssocs(dtm, c("programming" , "languages"), corlimit=0.85)
findAssocs(dtms, "human", corlimit=0.77) # specifying a correlation limit of 0.77
findAssocs(dtm, "human", corlimit=0.77) # specifying a correlation limit of 0.77
library(RColorBrewer)
set.seed(142)
wordcloud(names(freq), freq, min.freq=25)
wordcloud::names(freq), freq, min.freq=25)
wordcloud::wordcloud(names(freq), freq, min.freq=25)
save.image("H:/Mgr_Inf/Rstudio/LAB7/lab7-tekst_mining.RData")
savehistory("H:/Mgr_Inf/Rstudio/LAB7/lab7-tekst_mining.Rhistory")
